<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Object Detection & Tracking for Autonomous Driving</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- Bootstrap + FontAwesome -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet">
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/js/bootstrap.bundle.min.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.0/css/all.min.css">

    <style>
        body {
            background-color: #0d1117;
            color: #e6edf3;
            font-family: "Segoe UI", system-ui, sans-serif;
        }
        h1, h2, h3 {
            color: #58a6ff;
            font-weight: 700;
        }
        .hero {
            padding: 100px 0 60px;
            text-align: center;
        }
        .hero h1 {
            font-size: 3rem;
        }
        .btn-custom {
            background-color: #238636;
            border: none;
            color: white !important;
            padding: 10px 22px;
            border-radius: 8px;
            margin-right: 8px;
        }
        .btn-custom:hover { background-color: #2ea043; }
        hr { border-color: #30363d; }
        img { border-radius: 10px; margin-top: 12px; }
        iframe { border-radius: 10px; border: solid 1px #30363d; }
        .footer { text-align: center; padding: 40px; color: #8b949e; }
    </style>
</head>

<body>

    <!-- HERO -->
    <div class="hero container">
        <h1>Object Detection & Tracking System in Autonomous Driving</h1>
        <p class="mt-3 fs-5">Computer Vision Final Project</p>
        <p class="mt-4">
            <a href="poster.pdf" class="btn-custom"><i class="fa-solid fa-file-pdf"></i> Download Poster</a>
            <a href="https://github.com/YOUR_USERNAME/YOUR_REPO" class="btn-custom"><i class="fa-brands fa-github"></i> View Code</a>
        </p>
    </div>

    <hr class="container">

    <!-- ABSTRACT -->
    <div class="container py-5">
        <h2>Abstract</h2>
        <p>
            Object detection and tracking are essential perception tasks in autonomous driving, where the vehicle must recognize surrounding agents and maintain consistent identities over time. While many modern detectors—such as YOLO, RetinaNet, Faster R-CNN, and Mask R-CNN—are benchmarked extensively on static datasets, their influence on multi-object tracking performance is far less explored. 
        </p>
        <p>
            This project investigates this gap by fine-tuning four detectors on the KITTI Object Detection Dataset and evaluating them on the KITTI Tracking dataset under both pretrained and fine-tuned settings. Two tracking algorithms, DeepSORT and ByteTrack, are integrated to quantify how detection quality affects identity consistency and trajectory stability. 
        </p>
        <p>
            The results clearly show that stronger detectors yield more reliable tracking, fewer ID switches, and smoother trajectories—validating the hypothesis that “better tracking begins with better detection.”
        </p>
    </div>

    <hr class="container">

    <!-- INTRODUCTION -->
    <div class="container py-5">
        <h2>Introduction</h2>
        <p>
            Autonomous vehicles rely on precise perception to detect surrounding cars, pedestrians, and cyclists, and then track them across time to estimate motion. If detection and tracking pipelines fail to cooperate, the system may make incorrect trajectory predictions or perform unsafe maneuvers.
        </p>

        <p>
            While modern detectors achieve excellent accuracy on benchmarks, their downstream impact on multi-object tracking is rarely studied. Most research either evaluates detection or tracking independently, leaving a critical question unanswered:
        </p>

        <h4 class="text-info mt-3">“How does the choice of detector affect tracking stability in real driving scenes?”</h4>
    </div>

    <hr class="container">

    <!-- DATASET SECTION -->
    <div class="container py-5">
        <h2>Dataset Description</h2>

        <h3 class="mt-4">KITTI Object Detection Dataset</h3>
        <ul>
            <li>Training images: <strong>5984</strong></li>
            <li>Validation images: <strong>1497</strong></li>
            <li>Classes: <strong>Car, Pedestrian, Cyclist</strong></li>
            <li>Real-world driving scenes recorded in Karlsruhe, Germany</li>
        </ul>

        <img src="kitti_sample.png" class="img-fluid" alt="KITTI dataset sample placeholder">

        <h3 class="mt-5">KITTI Tracking Dataset</h3>
        <ul>
            <li>21 training sequences, 29 test sequences</li>
            <li>Frame-level annotations for multiple objects</li>
            <li>Used for benchmarking multi-object tracking (MOT)</li>
        </ul>
        <img src="kitti_tracking_sample.png" class="img-fluid" alt="KITTI tracking sample placeholder">
    </div>

    <hr class="container">

    <!-- RELATED WORK -->
    <div class="container py-5">
        <h2>Related Work</h2>

        <h3 class="mt-4">Object Detection</h3>
        <ul>
            <li><strong>YOLO11</strong> — real-time, one-stage, high recall</li>
            <li><strong>RetinaNet</strong> — Focal Loss for class imbalance</li>
            <li><strong>Faster R-CNN</strong> — two-stage, high accuracy</li>
            <li><strong>Mask R-CNN</strong> — segmentation + detection</li>
        </ul>

        <h3 class="mt-4">Object Tracking</h3>
        <ul>
            <li><strong>DeepSORT:</strong> appearance embedding + Kalman filter, sensitive to FP/FN</li>
            <li><strong>ByteTrack:</strong> high-performance tracker that smartly uses low-score detections</li>
        </ul>

        <h3 class="mt-4">Gap</h3>
        <p>
            Prior work rarely analyzes how detector choice affects tracking.  
            This project directly measures this relationship.
        </p>
    </div>

    <hr class="container">

    <!-- METHOD -->
    <div class="container py-5">
        <h2>Approach & Method</h2>

        <h3 class="mt-4 text-warning">Stage 1 — Detection Training</h3>
        <p>Four models fine-tuned on KITTI:</p>
        <ul>
            <li>YOLO11</li>
            <li>RetinaNet (R50-FPN)</li>
            <li>Faster R-CNN</li>
            <li>Mask R-CNN</li>
        </ul>

        <img src="detector_comparison.png" class="img-fluid" alt="Detector comparison placeholder">

        <h3 class="mt-5 text-warning">Stage 2 — Detection on KITTI Tracking</h3>
        <p>
            Both pretrained and fine-tuned models were applied to tracking frames.  
            Fine-tuned versions consistently improved mAP.
        </p>

        <h3 class="mt-5 text-warning">Stage 3 — Multi-Object Tracking (DeepSORT + ByteTrack)</h3>
        <p>
            Trackers were evaluated on outputs from both pretrained and fine-tuned YOLO11.
        </p>

        <img src="pipeline.png" class="img-fluid" alt="Pipeline diagram placeholder">
    </div>

    <hr class="container">

    <!-- NEW: TRACKING RESULT VIDEOS -->
    <div class="container py-5">
        <h2>Tracking Demonstration Videos</h2>
        <p class="text-secondary">These two videos clearly demonstrate how detection quality affects tracking stability.</p>

        <div class="mt-4">
            <h4 class="text-info">1️⃣ YOLO11 (Pretrained) + DeepSORT</h4>
            <p>Weak detections → ID jumps, fragmentation, missed tracks.</p>
            <video width="100%" controls>
                <source src="tracking_pretrained.mp4" type="video/mp4">
            </video>
        </div>

        <div class="mt-5">
            <h4 class="text-info">2️⃣ YOLO11 (Fine-Tuned) + DeepSORT</h4>
            <p>Much smoother ID consistency, fewer switches, more stable trajectories.</p>
            <video width="100%" controls>
                <source src="tracking_finetuned.mp4" type="video/mp4">
            </video>
        </div>
    </div>

    <hr class="container">

    <!-- RESULTS -->
    <div class="container py-5">
        <h2>Experiments & Results</h2>
        <p>Summary of key findings:</p>

        <ul>
            <li>YOLO11 fine-tuned achieves strongest mAP and fastest inference.</li>
            <li>Two-stage models: lower recall, slower, moderate tracking performance.</li>
            <li>Tracking improves significantly when using fine-tuned detections.</li>
        </ul>

        <img src="results.png" class="img-fluid" alt="Results placeholder">

        <h4 class="mt-5">Interactive 3D Viewer (Optional)</h4>
        <iframe width="100%" height="400" src="https://modelviewer.dev/shared-assets/models/Astronaut.glb"></iframe>
    </div>

    <hr class="container">

    <!-- DISCUSSION -->
    <div class="container py-5">
        <h2>Discussion & Limitations</h2>
        <ul>
            <li>Detection accuracy highly influences tracking robustness.</li>
            <li>DeepSORT fragile to false positives; ByteTrack more stable.</li>
            <li>Only KITTI dataset tested; domain shift not evaluated.</li>
        </ul>
    </div>

    <hr class="container">

    <!-- CONCLUSION -->
    <div class="container py-5">
        <h2>Conclusion</h2>
        <p>
            This project demonstrates that higher-quality detectors significantly improve multi-object tracking stability in autonomous driving. Fine-tuned YOLO11 consistently produced fewer ID switches and smoother trajectories, validating the hypothesis that “good tracking requires good detection.” The findings provide practical insights for building more reliable perception pipelines in real-world autonomous driving systems.
        </p>
    </div>

    <div class="footer">© 2025 | Computer Vision Final Project</div>

</body>
</html>
