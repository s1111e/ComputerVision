<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Object Detection & Tracking in Autonomous Driving</title>

    <!-- Bootstrap -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.2/dist/css/bootstrap.min.css" rel="stylesheet">

    <!-- Custom Style -->
    <style>
        body { background: #f8f9fa; }
        .section-title { font-weight: 700; font-size: 2rem; margin-top: 50px; }
        .subsection-title { font-weight: 600; font-size: 1.4rem; margin-top: 25px; }
        .img-block { width: 100%; border-radius: 10px; box-shadow: 0 2px 12px rgba(0,0,0,0.15); }
        .img-block-mini { width: 30%; border-radius: 10px; box-shadow: 0 2px 12px rgba(0,0,0,0.15); }
        .video-block { width: 100%; border-radius: 10px; margin: 10px 0; }
        .flowchart { width: 100%; padding: 10px; border-radius: 12px; background: white; }
    </style>
</head>

<body>

<!-- HERO -->
<header class="bg-dark text-white text-center py-5">
    <h1 class="display-4 fw-bold">Object Detection & Tracking System in Autonomous Driving</h1>
    <p class="lead">A Comparative Study of YOLO, RetinaNet, Faster R-CNN, Mask R-CNN + DeepSORT </p>
</header>

<div class="container py-4">

<!-- ABSTRACT -->
<section id="abstract">
    <h2 class="section-title">Abstract</h2>
    <p>
        This project investigates object detection and tracking performance in autonomous driving scenarios.
        Four detection models (YOLO11, RetinaNet, Faster R-CNN, Mask R-CNN) were fine-tuned on the KITTI object
        detection dataset and evaluated both on the KITTI detection and KITTI tracking dataset. Two tracking
        algorithms, DeepSORT, were used to analyze how detection accuracy affects multi-object tracking
        stability. Experimental results show that fine-tuned detectors significantly reduce ID switching and tracking
        fragmentation compared to pretrained models, confirming the strong coupling between detection quality and
        overall tracking performance.
    </p>
</section>

<!-- INTRODUCTION -->
<section id="intro">
    <h2 class="section-title">Introduction</h2>
    <p>
        Autonomous vehicles require robust detection and tracking of road users such as cars, pedestrians, and cyclists.
        While modern detectors achieve high accuracy on static images, their real-world performance within tracking
        pipelines is less explored. This project addresses the research gap:
        <strong>“How does the choice and fine-tuning of object detectors affect multi-object tracking performance?”</strong>
    </p>
</section>

<!-- DATASET -->
<section id="dataset">
    <h2 class="section-title">Dataset</h2>
    <h4 class="subsection-title">Why KITTI?</h4>
    <p>
      The <strong>KITTI Vision Benchmark Suite</strong> provides synchronized multi-sensor data from real driving scenarios: stereo RGB cameras, a 64-beam LiDAR scanner, and GPS/IMU for vehicle localization.  
      This rich sensor setup allows tasks beyond 2D detection — such as 3D object detection, depth estimation, odometry, and even sensor fusion studies. <br>
      The dataset is widely adopted in autonomous driving research, ensuring that results are comparable to the literature.
    </p>
    <img src="images/kitti.png" class="img-block-mini my-3" alt="KITTI detection sample">

    <h4 class="subsection-title">Dataset Structure & Contents</h4>
    <ul>
      <li><strong>Object Detection Benchmark:</strong> ~7481 training + ~7518 test images, annotated in 2D bounding-boxes + 3D box metadata. Class labels include Cars, Pedestrians, Cyclists, Trucks, Vans, Trams, etc.</li>
      <li><strong>Object Tracking Benchmark:</strong> 21 annotated sequences for training, 29 for testing. Ground-truth includes object track IDs, occlusion / truncation metadata, and 2D bounding boxes for each frame.</li>
      <li>Only “Car” and “Pedestrian” categories are evaluated in the official tracking benchmark to ensure sufficient data per class.</li>
      <li>Annotation format provides detailed information: occlusion level, truncation ratio, visibility, 3D position & size — enabling tasks beyond simple 2D detection.</li>
    </ul>

    <h4 class="subsection-title">KITTI Detection Dataset</h4>
    <ul>
        <li>Train: 5984 images</li>
        <li>Validation: 1497 images</li>
        <li>Classes: Car, Pedestrian, Cyclist</li>
    </ul>

    <img src="images/kitti_detection_sample1.png" class="img-block my-3" alt="KITTI detection sample">

    <h4 class="subsection-title">KITTI Tracking Dataset</h4>
    <ul>
        <li>20 sequences</li>
        <li>Variable frame count per sequence</li>
        <li>Used for Testing Detection & Tracking</li>
    </ul>

    <img src="images/kitti_tracking_sample.png" class="img-block my-3" alt="KITTI tracking sample">
</section>

<!-- MODELS -->
<section id="models">
    <h2 class="section-title">Object Detection Models</h2>

    <p>
        Modern autonomous driving systems require fast and accurate object detectors capable of identifying multiple
        classes across varying lighting, occlusion, and traffic conditions. In this project, we compare four
        state-of-the-art detection algorithms that represent both <strong>one-stage</strong> and <strong>two-stage</strong>
        detection paradigms. This selection allows us to analyze differences in accuracy, speed, complexity, and their
        impact on downstream tracking algorithms such as DeepSORT and ByteTrack.
    </p>

    <!-- MODEL IMAGES -->
    <div class="row text-center my-4">
        <div class="col-md-3">
            <img src="images/yolo_arch.png" class="img-block" alt="YOLO">
            <h5 class="mt-2">YOLO11 (One-Stage)</h5>
        </div>
        <div class="col-md-3">
            <img src="images/retina_arch.png" class="img-block" alt="RetinaNet">
            <h5 class="mt-2">RetinaNet (One-Stage)</h5>
        </div>
        <div class="col-md-3">
            <img src="images/frcnn_arch.png" class="img-block" alt="FRCNN">
            <h5 class="mt-2">Faster R-CNN (Two-Stage)</h5>
        </div>
        <div class="col-md-3">
            <img src="images/mrcnn_arch.png" class="img-block" alt="Mask R-CNN">
            <h5 class="mt-2">Mask R-CNN (Two-Stage)</h5>
        </div>
    </div>


    <!-- MODEL DESCRIPTIONS -->
    <h3 class="subsection-title">Model Descriptions & Motivation</h3>

    <h4 class="mt-4">1. YOLO11 (One-Stage Detector)</h4>
    <p>
        YOLO (“You Only Look Once”) is a real-time object detector that processes the entire image in a single forward pass. 
        YOLO11 represents the latest generation of this architecture, offering improved feature fusion, optimized anchor-free heads,
        and high-speed inference ideal for real-time autonomous driving.
        <br><strong>Why included?</strong> It provides the best speed–accuracy trade-off and is widely used in tracking pipelines.
    </p>

    <h4 class="mt-4">2. RetinaNet (One-Stage Detector)</h4>
    <p>
        RetinaNet introduced the <strong>Focal Loss</strong>, solving the class imbalance problem common in autonomous driving scenes 
        (many cars, few pedestrians/cyclists). It uses a Feature Pyramid Network (FPN) to detect small and medium objects more reliably.
        <br><strong>Why included?</strong> Represents earlier SOTA single-stage detectors and provides a strong baseline against YOLO.
    </p>

    <h4 class="mt-4">3. Faster R-CNN (Two-Stage Detector)</h4>
    <p>
        Faster R-CNN is a classic high-accuracy detector that uses a <strong>Region Proposal Network (RPN)</strong> to generate regions 
        before classification. It is slower but significantly more stable in complex scenes.
        <br><strong>Why included?</strong> Serves as a two-stage accuracy-focused baseline for comparison with one-stage methods.
    </p>

    <h4 class="mt-4">4. Mask R-CNN (Two-Stage Detector)</h4>
    <p>
        Mask R-CNN extends Faster R-CNN by adding an <strong>instance segmentation branch</strong>, improving spatial precision at 
        object boundaries. Even though this project uses only bounding boxes, the shared backbone provides strong feature representations.
        <br><strong>Why included?</strong> Represents more advanced two-stage models with higher modeling capacity.
    </p>


    <!-- COMPARISON TABLE -->
    <h3 class="subsection-title mt-5">Model Comparison Summary</h3>
    <table class="table table-bordered table-striped">
        <thead>
            <tr>
                <th>Model</th>
                <th>Type</th>
                <th>Speed</th>
                <th>Accuracy</th>
                <th>Strengths</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td>YOLO11</td>
                <td>One-Stage</td>
                <td>⭐⭐⭐⭐⭐</td>
                <td>⭐⭐⭐⭐</td>
                <td>Real-time, most stable for tracking</td>
            </tr>
            <tr>
                <td>RetinaNet</td>
                <td>One-Stage</td>
                <td>⭐⭐⭐</td>
                <td>⭐⭐⭐</td>
                <td>Focal Loss improves minority classes</td>
            </tr>
            <tr>
                <td>Faster R-CNN</td>
                <td>Two-Stage</td>
                <td>⭐⭐</td>
                <td>⭐⭐⭐⭐⭐</td>
                <td>Highly accurate proposals</td>
            </tr>
            <tr>
                <td>Mask R-CNN</td>
                <td>Two-Stage</td>
                <td>⭐</td>
                <td>⭐⭐⭐⭐⭐</td>
                <td>Best for occlusion & boundary precision</td>
            </tr>
        </tbody>
    </table>


    <!-- LOSS CURVES -->
    <h3 class="subsection-title mt-5">Training Loss Curves</h3>
    <p>
        Loss curves demonstrate whether each model converged properly during fine-tuning on the KITTI detection dataset.
        Models with steadily decreasing loss show stable learning and good generalization to validation data.
    </p>

    <div class="row text-center">
        <div class="col-md-3"><img src="images/yolo_loss.png" class="img-block"></div>
        <div class="col-md-3"><img src="images/retina_loss.png" class="img-block"></div>
        <div class="col-md-3"><img src="images/frcnn_loss.png" class="img-block"></div>
        <div class="col-md-3"><img src="images/mrcnn_loss.png" class="img-block"></div>
    </div>
</section>


<!-- DETECTION RESULTS -->
<section id="detection-results">
    <h2 class="section-title">Detection Results (KITTI)</h2>

    <h4 class="subsection-title">mAP Performance</h4>
    <p><strong>YOLO11 fine-tuned (given results):</strong></p>

    <table class="table table-bordered table-striped">
        <thead>
            <tr>
                <th>Model</th>
                <th>mAP50-95</th>
                <th>mAP50</th>
                <th>Car AP</th>
                <th>Pedestrian AP</th>
                <th>Cyclist AP</th>
            </tr>
        </thead>

        <tbody>
            <tr>
                <td>YOLO11 (FT)</td>
                <td>0.494</td>
                <td>0.765</td>
                <td>0.702</td>
                <td>0.369</td>
                <td>0.411</td>
            </tr>

            <!-- EMPTY ROWS FOR FUTURE RESULTS -->
            <tr><td>YOLO11 (Pretrained)</td><td colspan="5">Pending...</td></tr>
            <tr><td>RetinaNet (FT)</td><td colspan="5">Pending...</td></tr>
            <tr><td>Faster R-CNN (FT)</td><td colspan="5">Pending...</td></tr>
            <tr><td>Mask R-CNN (FT)</td><td colspan="5">Pending...</td></tr>
        </tbody>
    </table>

    <h4 class="subsection-title">Detection Examples</h4>
    <div class="row">
        <div class="col-md-3"><img src="images/yolo_detection.png" class="img-block"></div>
        <div class="col-md-3"><img src="images/retina_detection.png" class="img-block"></div>
        <div class="col-md-3"><img src="images/frcnn_detection.png" class="img-block"></div>
        <div class="col-md-3"><img src="images/mrcnn_detection.png" class="img-block"></div>
    </div>
</section>

<!-- TRACKING PIPELINE -->
<section id="tracking">
    <h2 class="section-title">Tracking Pipeline</h2>

    <img src="images/tracking_flowchart.svg" class="flowchart" alt="Tracking Flowchart">

    <h4 class="subsection-title">Tracker Overview</h4>
    <p>
        Two state-of-the-art multi-object tracking algorithms were used:
    </p>
    <ul>
        <li><strong>DeepSORT</strong>: Appearance-based tracking using ReID + Kalman filter + Hungarian matching</li>
    </ul>
</section>

<!-- TRACKING RESULTS -->
<section id="tracking-results">
    <h2 class="section-title">Tracking Results</h2>

    <h3 class="subsection-title">Qualitative Comparison</h3>

    <h5>YOLO11 (Pretrained) + DeepSORT</h5>
    <video class="video-block" controls>
        <source src="images/yolo_pretrained_deepsort.mp4" type="video/mp4">
    </video>

    <h5>YOLO11 (Fine-Tuned) + DeepSORT</h5>
    <video class="video-block" controls>
        <source src="images/yolo_finetuned_deepsort.mp4" type="video/mp4">
    </video>

    <img src="images/tracking_compare.png" class="img-block mt-3">

    <h3 class="subsection-title">Tracking Metrics</h3>

    <table class="table table-bordered">
        <thead>
            <tr>
                <th>Setup</th>
                <th>ID Switch</th>
                <th>Fragmentation</th>
                <th>FP</th>
                <th>FN</th>
                <th>Recall</th>
                <th>Precision</th>
            </tr>
        </thead>
        <tbody>
            <tr><td>YOLO Pretrained + DeepSORT</td><td colspan="6">Pending...</td></tr>
            <tr><td>YOLO Fine-Tuned + DeepSORT</td><td colspan="6">Pending...</td></tr>
        </tbody>
    </table>
</section>

<!-- CONCLUSION -->
<section id="conclusion">
    <h2 class="section-title">Conclusion</h2>
    <p>
        Fine-tuned detection models significantly improve multi-object tracking quality in autonomous driving data.
        YOLO11 achieved the best overall performance, and when used with DeepSORT, it reduced ID switching and improved
        track continuity. The results confirm that high-quality detections directly lead to better tracking stability.
    </p>
</section>

<!-- REFERENCES -->
<section id="references">
    <h2 class="section-title">References</h2>
    <ul>
        <li>Redmon et al., YOLO</li>
        <li>Lin et al., Focal Loss, RetinaNet</li>
        <li>Ren et al., Faster R-CNN</li>
        <li>He et al., Mask R-CNN</li>
        <li>Wojke et al., DeepSORT</li>
        <li>ByteTrack (2022)</li>
        <li>KITTI Dataset</li>
    </ul>
</section>

<!-- FOOTER -->
<footer class="text-center py-4 mt-5">
    <p>© 2025 Object Detection & Tracking Research Project</p>
</footer>

</div>

</body>
</html>
