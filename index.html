<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Object Detection & Tracking in Autonomous Driving</title>

    <!-- Bootstrap -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.2/dist/css/bootstrap.min.css" rel="stylesheet">

    <!-- Custom Style -->
    <style>
        body { background: #f8f9fa; }
        .section-title { font-weight: 700; font-size: 2rem; margin-top: 50px; }
        .subsection-title { font-weight: 600; font-size: 1.4rem; margin-top: 25px; }
        .img-block { width: 100%; border-radius: 10px; box-shadow: 0 2px 12px rgba(0,0,0,0.15); }
        .img-block-mini { width: 30%; border-radius: 10px; box-shadow: 0 2px 12px rgba(0,0,0,0.15); }
        .video-block { width: 100%; border-radius: 10px; margin: 10px 0; }
        .flowchart { width: 100%; padding: 10px; border-radius: 12px; background: white; }
    </style>
</head>

<body>

<!-- HERO -->
<header class="bg-dark text-white text-center py-5">
    <h1 class="display-4 fw-bold">Object Detection & Tracking System in Autonomous Driving</h1>
    <p class="lead">A Comparative Study of YOLO, RetinaNet, Faster R-CNN, Mask R-CNN + DeepSORT </p>

    
    <p class="lead"> CS 5243 - Computer Vision </p>

    <p class="lead"> Project Group 18 </p>
</header>

<div class="container py-4">

<!-- ABSTRACT -->
<section id="abstract">
    <h2 class="section-title">Abstract</h2>
    <p>
        This project investigates object detection and tracking performance in autonomous driving scenarios.
        Four detection models (YOLO11, RetinaNet, Faster R-CNN, Mask R-CNN) were fine-tuned on the KITTI object
        detection dataset and evaluated both on the KITTI detection and KITTI tracking dataset. Two tracking
        algorithms, DeepSORT, were used to analyze how detection accuracy affects multi-object tracking
        stability. Experimental results show that fine-tuned detectors significantly reduce ID switching and tracking
        fragmentation compared to pretrained models, confirming the strong coupling between detection quality and
        overall tracking performance.
    </p>
</section>

<!-- INTRODUCTION -->
<section id="intro">
    <h2 class="section-title">Introduction</h2>
    <p>
        Autonomous vehicles require robust detection and tracking of road users such as cars, pedestrians, and cyclists.
        While modern detectors achieve high accuracy on static images, their real-world performance within tracking
        pipelines is less explored. This project addresses the research gap:
        <strong>‚ÄúHow does the choice and fine-tuning of object detectors affect multi-object tracking performance?‚Äù</strong>
    </p>
</section>

<!-- DATASET -->
<section id="dataset">
    <h2 class="section-title">Dataset</h2>
    <h4 class="subsection-title">Why KITTI?</h4>
    <p>
      The <strong>KITTI Vision Benchmark Suite</strong> provides synchronized multi-sensor data from real driving scenarios: stereo RGB cameras, a 64-beam LiDAR scanner, and GPS/IMU for vehicle localization.  
      This rich sensor setup allows tasks beyond 2D detection ‚Äî such as 3D object detection, depth estimation, odometry, and even sensor fusion studies. <br>
      The dataset is widely adopted in autonomous driving research, ensuring that results are comparable to the literature.
    </p>
    <img src="images/kitti.png" class="img-block-mini my-3" alt="KITTI detection sample">

    <h4 class="subsection-title">Dataset Structure & Contents</h4>
    <ul>
      <li><strong>Object Detection Benchmark:</strong> ~7481 training + ~7518 test images, annotated in 2D bounding-boxes + 3D box metadata. Class labels include Cars, Pedestrians, Cyclists, Trucks, Vans, Trams, etc.</li>
      <li><strong>Object Tracking Benchmark:</strong> 21 annotated sequences for training, 29 for testing. Ground-truth includes object track IDs, occlusion / truncation metadata, and 2D bounding boxes for each frame.</li>
      <li>Only ‚ÄúCar‚Äù and ‚ÄúPedestrian‚Äù categories are evaluated in the official tracking benchmark to ensure sufficient data per class.</li>
      <li>Annotation format provides detailed information: occlusion level, truncation ratio, visibility, 3D position & size ‚Äî enabling tasks beyond simple 2D detection.</li>
    </ul>

    <h4 class="subsection-title">KITTI Detection Dataset</h4>
    <ul>
        <li>Train: 5984 images</li>
        <li>Validation: 1497 images</li>
        <li>Classes: Car, Pedestrian, Cyclist</li>
    </ul>

    <img src="images/kitti_detection_sample1.png" class="img-block my-3" alt="KITTI detection sample">

    <h4 class="subsection-title">KITTI Tracking Dataset</h4>
    <ul>
        <li>20 sequences</li>
        <li>Variable frame count per sequence</li>
        <li>Used for Testing Detection & Tracking</li>
    </ul>

    <img src="images/kitti_tracking_sample.png" class="img-block my-3" alt="KITTI tracking sample">
</section>

<!-- MODELS -->
<section id="models">
    <h2 class="section-title">Object Detection Models</h2>

    <p>
        Modern autonomous driving systems require fast and accurate object detectors capable of identifying multiple
        classes across varying lighting, occlusion, and traffic conditions. In this project, we compare four
        state-of-the-art detection algorithms that represent both <strong>one-stage</strong> and <strong>two-stage</strong>
        detection paradigms. This selection allows us to analyze differences in accuracy, speed, complexity, and their
        impact on downstream tracking algorithms such as DeepSORT and ByteTrack.
    </p>

    <!-- MODEL IMAGES -->
    <div class="row text-center my-4">
        <div class="col-md-3">
            <img src="images/yolo_arch.png" class="img-block" alt="YOLO">
            <h5 class="mt-2">YOLO11 (One-Stage)</h5>
        </div>
        <div class="col-md-3">
            <img src="images/retina_arch.png" class="img-block" alt="RetinaNet">
            <h5 class="mt-2">RetinaNet (One-Stage)</h5>
        </div>
        <div class="col-md-3">
            <img src="images/frcnn_arch.png" class="img-block" alt="FRCNN">
            <h5 class="mt-2">Faster R-CNN (Two-Stage)</h5>
        </div>
        <div class="col-md-3">
            <img src="images/mrcnn_arch.png" class="img-block" alt="Mask R-CNN">
            <h5 class="mt-2">Mask R-CNN (Two-Stage)</h5>
        </div>
    </div>


    <!-- MODEL DESCRIPTIONS -->
    <h3 class="subsection-title">Model Descriptions & Motivation</h3>

    <h4 class="mt-4">1. YOLO11 (One-Stage Detector)</h4>
    <p>
        YOLO (‚ÄúYou Only Look Once‚Äù) is a real-time object detector that processes the entire image in a single forward pass. 
        YOLO11 represents the latest generation of this architecture, offering improved feature fusion, optimized anchor-free heads,
        and high-speed inference ideal for real-time autonomous driving.
        <br><strong>Why included?</strong> It provides the best speed‚Äìaccuracy trade-off and is widely used in tracking pipelines.
    </p>

    <h4 class="mt-4">2. RetinaNet (One-Stage Detector)</h4>
    <p>
        RetinaNet introduced the <strong>Focal Loss</strong>, solving the class imbalance problem common in autonomous driving scenes 
        (many cars, few pedestrians/cyclists). It uses a Feature Pyramid Network (FPN) to detect small and medium objects more reliably.
        <br><strong>Why included?</strong> Represents earlier SOTA single-stage detectors and provides a strong baseline against YOLO.
    </p>

    <h4 class="mt-4">3. Faster R-CNN (Two-Stage Detector)</h4>
    <p>
        Faster R-CNN is a classic high-accuracy detector that uses a <strong>Region Proposal Network (RPN)</strong> to generate regions 
        before classification. It is slower but significantly more stable in complex scenes.
        <br><strong>Why included?</strong> Serves as a two-stage accuracy-focused baseline for comparison with one-stage methods.
    </p>

    <h4 class="mt-4">4. Mask R-CNN (Two-Stage Detector)</h4>
    <p>
        Mask R-CNN extends Faster R-CNN by adding an <strong>instance segmentation branch</strong>, improving spatial precision at 
        object boundaries. Even though this project uses only bounding boxes, the shared backbone provides strong feature representations.
        <br><strong>Why included?</strong> Represents more advanced two-stage models with higher modeling capacity.
    </p>


    <!-- COMPARISON TABLE -->
    <h3 class="subsection-title mt-5">Model Comparison Summary</h3>
    <table class="table table-bordered table-striped">
        <thead>
            <tr>
                <th>Model</th>
                <th>Type</th>
                <th>Speed</th>
                <th>Accuracy</th>
                <th>Strengths</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td>YOLO11</td>
                <td>One-Stage</td>
                <td>‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê</td>
                <td>‚≠ê‚≠ê‚≠ê‚≠ê</td>
                <td>Real-time, most stable for tracking</td>
            </tr>
            <tr>
                <td>RetinaNet</td>
                <td>One-Stage</td>
                <td>‚≠ê‚≠ê‚≠ê</td>
                <td>‚≠ê‚≠ê‚≠ê</td>
                <td>Focal Loss improves minority classes</td>
            </tr>
            <tr>
                <td>Faster R-CNN</td>
                <td>Two-Stage</td>
                <td>‚≠ê‚≠ê</td>
                <td>‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê</td>
                <td>Highly accurate proposals</td>
            </tr>
            <tr>
                <td>Mask R-CNN</td>
                <td>Two-Stage</td>
                <td>‚≠ê</td>
                <td>‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê</td>
                <td>Best for occlusion & boundary precision</td>
            </tr>
        </tbody>
    </table>


    <!-- LOSS CURVES -->
    <h3 class="subsection-title mt-5">Training Loss Curves</h3>
    <p>
        Loss curves demonstrate whether each model converged properly during fine-tuning on the KITTI detection dataset.
        Models with steadily decreasing loss show stable learning and good generalization to validation data.
    </p>

    <div class="row text-center">
        <div class="col-md-3"><img src="images/yolo_loss.png" class="img-block"></div>
        <div class="col-md-3"><img src="images/retina_loss.png" class="img-block"></div>
        <div class="col-md-3"><img src="images/frcnn_loss.png" class="img-block"></div>
        <div class="col-md-3"><img src="images/mrcnn_loss.png" class="img-block"></div>
    </div>
</section>

<!-- DETECTION RESULTS -->
<section id="detection-results">
    <h2 class="section-title">Detection Results</h2>

    <p class="section-desc">
        We evaluate four modern object detection architectures on the KITTI dataset. 
        Each model is tested under two settings: 
        <strong>Pretrained (COCO weights applied directly on KITTI Tracking)</strong> and 
        <strong>Fine-Tuned (trained on KITTI Detection dataset)</strong>. 
        This section provides a comparative analysis of model behavior, strengths, weaknesses, 
        and how fine-tuning influences downstream tracking.
    </p>

    <!-- PRETRAINED RESULTS -->
    <h4 class="subsection-title">üîµ Pretrained Detection Performance (Direct Evaluation on KITTI)</h4>
    <p class="subsection-desc">
        These results show how well each model performs <strong>without any KITTI-specific training</strong>.  
        This reflects generalization ability from COCO ‚Üí Autonomous driving domain.
    </p>

    <table class="table table-bordered table-striped">
        <thead>
            <tr>
                <th>Model</th>
                <th>mAP50</th>
                <th>Summary</th>
            </tr>
        </thead>

        <tbody>
            <tr>
                <td><strong>YOLO11 (Pretrained)</strong></td>
                <td>0.371</td>
                <td>Best generalization; high recall on cars.</td>
            </tr>

            <tr>
                <td><strong>RetinaNet (Pretrained)</strong></td>
                <td>0.236</td>
                <td>Struggles with small objects; moderate precision.</td>
            </tr>

            <tr>
                <td><strong>Faster R-CNN (Pretrained)</strong></td>
                <td>0.211</td>
                <td>High precision but weak recall; misses distant objects.</td>
            </tr>

            <tr>
                <td><strong>Mask R-CNN (Pretrained)</strong></td>
                <td>0.214</td>
                <td>Mask head disabled; similar to FRCNN baseline.</td>
            </tr>
        </tbody>
    </table>

    <div class="analysis-box">
        <h5>Interpretation</h5>
        <ul>
            <li><strong>YOLO11 clearly generalizes better</strong> to driving scenes even without fine-tuning.</li>
            <li><strong>Two-stage models (FRCNN, MRCNN)</strong> require dataset-specific tuning before performing well.</li>
            <li><strong>RetinaNet</strong> sits in the middle: acceptable precision but poor KITTI recall.</li>
            <li>These results confirm that pretrained models alone are <strong>not sufficient for tracking</strong> ‚Äî especially in structured autonomous driving scenarios.</li>
        </ul>
    </div>

    <hr>

    <!-- FINE-TUNED RESULTS -->
    <h4 class="subsection-title">üîµ Fine-Tuned Detection Performance (KITTI Object Detection)</h4>
    <p class="subsection-desc">
        After training each model on the KITTI Detection dataset, performance improves significantly.  
        These results better represent how the detector behaves in the final detection‚Äìtracking system.
    </p>

    <table class="table table-bordered table-striped">
        <thead>
            <tr>
                <th>Model</th>
                <th>mAP50-95</th>
                <th>mAP50</th>
                <th>Car AP</th>
                <th>Pedestrian AP</th>
                <th>Cyclist AP</th>
            </tr>
        </thead>

        <tbody>
            <!-- YOLO11 FT -->
            <tr>
                <td><strong>YOLO11 (Fine-Tuned)</strong></td>
                <td>0.494</td>
                <td>0.765</td>
                <td>0.702</td>
                <td>0.369</td>
                <td>0.411</td>
            </tr>

            <!-- RetinaNet FT -->
            <tr>
                <td><strong>RetinaNet (Fine-Tuned)</strong></td>
                <td>0.32</td>
                <td>0.60</td>
                <td>0.63</td>
                <td>0.28</td>
                <td>0.29</td>
            </tr>

            <!-- Faster R-CNN FT -->
            <tr>
                <td><strong>Faster R-CNN (Fine-Tuned)</strong></td>
                <td>0.35</td>
                <td>0.62</td>
                <td>0.66</td>
                <td>0.30</td>
                <td>0.31</td>
            </tr>

            <!-- Mask R-CNN FT -->
            <tr>
                <td><strong>Mask R-CNN (Fine-Tuned)</strong></td>
                <td>0.34</td>
                <td>0.61</td>
                <td>0.65</td>
                <td>0.29</td>
                <td>0.30</td>
            </tr>
        </tbody>
    </table>

    <div class="analysis-box">
        <h5>Key Observations</h5>
        <ul>
            <li><strong>YOLO11 achieves the highest mAP50 and overall accuracy</strong>, making it the best detector for real-time tracking scenarios.</li>
            <li><strong>Fine-tuning dramatically boosts performance</strong> for all models ‚Äî especially two-stage detectors.</li>
            <li><strong>FRCNN and MRCNN</strong> achieve competitive performance but remain slower during inference.</li>
            <li><strong>RetinaNet</strong> improves substantially but still falls behind in pedestrian recall.</li>
            <li>These results show a clear trend: <strong>better detection ‚Üí more stable tracking (fewer ID switches)</strong>.</li>
        </ul>
    </div>

</section>

<style>
.section-title { 
    font-weight: 700; 
    margin-top: 40px; 
}
.subsection-title { 
    margin-top: 30px; 
    font-weight: 600; 
}
.section-desc, .subsection-desc { 
    color: #444; 
    margin-bottom: 20px; 
}
.analysis-box {
    background: #f7f9fc;
    border-left: 4px solid #007bff;
    padding: 15px 20px;
    margin-top: 20px;
    margin-bottom: 20px;
    border-radius: 6px;
}
.analysis-box h5 {
    font-weight: 700;
}
</style>




    

<!-- TRACKING PIPELINE -->
<section id="tracking">
    <h2 class="section-title">Tracking Pipeline</h2>

    <img src="images/tracking_flowchart.svg" class="flowchart" alt="Tracking Flowchart">

    <h4 class="subsection-title">Overview</h4>
    <p>
        After evaluating four object detection models on KITTI Tracking, we selected two versions of YOLO11 
        (pretrained and fine-tuned) for multi-object tracking experiments. The goal was to analyze how detection
        quality influences the performance of a tracking algorithm in real driving scenes.
    </p>

    <h4 class="subsection-title">Why Multi-Object Tracking?</h4>
    <p>
        Autonomous vehicles must track the same object across consecutive frames. While object detection identifies 
        objects independently per image, tracking links detections over time to maintain consistent object identities.
        Tracking performance is highly sensitive to detection quality‚Äîpoor detections lead to identity switches,
        false positives, and unstable trajectories.
    </p>

    <h4 class="subsection-title">Tracking Algorithm: DeepSORT</h4>
    <p>
        We employed <strong>DeepSORT</strong>, a widely used and reliable MOT algorithm. DeepSORT extends SORT by 
        adding appearance-based re-identification (ReID), making it more robust in crowded or occluded environments.
    </p>

    <ul>
        <li><strong>Appearance Embeddings (ReID):</strong> Each detected object is converted to a feature vector for identity matching.</li>
        <li><strong>Kalman Filter:</strong> Predicts object motion across frames even when detections are missing.</li>
        <li><strong>Hungarian Algorithm:</strong> Matches predictions with detections via IoU + appearance similarity.</li>
        <li><strong>ID Assignment:</strong> Maintains consistent ID numbers over time.</li>
    </ul>

    <p>
        DeepSORT is a strong baseline for autonomous driving systems due to its speed, simplicity, and reliable 
        identity preservation‚Äîespecially important for safety-critical tasks.
    </p>

    <h4 class="subsection-title">Experiment Setup</h4>
    <p>
        We tested the tracker on KITTI Tracking Sequence <strong>0001</strong>, once using:
    </p>

    <ul>
        <li><strong>YOLO11 Pretrained</strong> detections</li>
        <li><strong>YOLO11 Fine-Tuned</strong> detections</li>
    </ul>

    <p>
        This allowed a controlled comparison of how fine-tuning detector models directly affects tracking behavior:
        fewer false detections ‚Üí more accurate trajectories ‚Üí reduced ID switches.
    </p>
</section>


<!-- TRACKING RESULTS -->
<section id="tracking-results">
    <h2 class="section-title">Tracking Results</h2>

    <h3 class="subsection-title">Qualitative Comparison (Sequence 0001)</h3>
    <p>
        Below are the tracking visualization videos generated from KITTI 0001. The improvement after fine-tuning 
        is clearly visible‚ÄîIDs remain stable, tracks do not jump, and the tracker loses fewer objects.
    </p>

    <h5>YOLO11 (Pretrained) + DeepSORT</h5>
    <video class="video-block" controls autoplay>
        <source src="images/yolo_pretrained_deepsort.mp4" type="video/mp4">
    </video>

    <h5>YOLO11 (Fine-Tuned) + DeepSORT</h5>
    <video class="video-block" controls autoplay>
        <source src="images/yolo_finetuned_deepsort.mp4" type="video/mp4">
    </video>

    <img src="images/tracking_compare.png" class="img-block mt-3">

   <h3 class="subsection-title">Quantitative Metrics</h3>

<p>
    We evaluated the tracking performance on KITTI Sequence 0001 using standard Multi-Object Tracking (MOT) metrics.  
    These metrics quantify how well the tracker maintains object identities, minimizes false detections, and recovers 
    objects across frames. The results clearly show that <strong>fine-tuning the detector dramatically improves tracking performance</strong>.
</p>

<ul>
    <li><strong>MOTA (Multiple Object Tracking Accuracy):</strong> penalizes False Positives (FP), False Negatives (FN), and ID switches.</li>
    <li><strong>IDF1:</strong> measures how consistently object identities are preserved over time.</li>
    <li><strong>ID Switches:</strong> lower values indicate better identity stability.</li>
    <li><strong>FP / FN:</strong> false detections and missed detections.</li>
    <li><strong>Precision / Recall:</strong> overall detection quality during tracking.</li>
</ul>

<table class="table table-bordered">
    <thead>
        <tr>
            <th>Setup</th>
            <th>ID Switch</th>
            <th>FP</th>
            <th>FN</th>
            <th>Recall</th>
            <th>Precision</th>
            <th>MOTA</th>
            <th>IDF1</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>YOLO11 Pretrained + DeepSORT</td>
            <td>176</td>
            <td>6560</td>
            <td>2081</td>
            <td>0.4873</td>
            <td>0.2317</td>
            <td>-1.9099</td>
            <td>0.314</td>
        </tr>

        <tr>
            <td>YOLO11 Fine-Tuned + DeepSORT</td>
            <td>49</td>
            <td>3206</td>
            <td>1771</td>
            <td>0.5438</td>
            <td>0.3970</td>
            <td>-0.6587</td>
            <td>0.459</td>
        </tr>
    </tbody>
</table>

<p>
    The fine-tuned YOLO11 model provides a much cleaner input to the tracker:
</p>

<ul>
    <li><strong>ID switches dropped from 176 ‚Üí 49</strong> (a 72% improvement), meaning identities remain stable.</li>
    <li><strong>False Positives reduced by 51%</strong> (6560 ‚Üí 3206), avoiding phantom detections.</li>
    <li><strong>False Negatives reduced</strong> (2081 ‚Üí 1771), so fewer objects are missed.</li>
    <li><strong>Precision improved from 0.23 ‚Üí 0.40</strong>, meaning detections are more reliable.</li>
    <li><strong>Recall improved from 0.49 ‚Üí 0.54</strong>, indicating more objects are successfully tracked.</li>
    <li><strong>IDF1 improved from 0.314 ‚Üí 0.459</strong>, showing stronger identity consistency.</li>
</ul>

<p>
    Although MOTA remains negative (common on KITTI when FP is high), the improvement from <strong>-1.91 to -0.65</strong>
    indicates that the tracker benefits substantially from the cleaner detections of the fine-tuned model.
</p>

     <div class="row text-center">
        <div class="col-md-4"><img src="images/compare1.png" class="img-block"></div>
        <div class="col-md-4"><img src="images/compare2.png" class="img-block"></div>
        <div class="col-md-4"><img src="images/compare3.png" class="img-block"></div>
    </div>
    
<p>
    Overall, these results confirm that <strong>detection quality is the dominant factor in multi-object tracking</strong>.  
    Fine-tuned YOLO11 produces more accurate bounding boxes and fewer noisy detections, resulting in significantly 
    more stable and reliable tracking performance.
</p>


    <p>
        Fine-tuning the detector reduced ID switches by <strong>~72%</strong> and significantly improved precision and IDF1, 
        showing that stronger detections directly improve tracking stability.
    </p>
</section>

<!-- CONCLUSION -->
<!-- CONCLUSION -->
<section id="conclusion">
    <h2 class="section-title">Conclusion</h2>

    <p>
        This project demonstrates that high-quality object detection is the most important factor affecting the 
        performance of multi-object tracking in autonomous driving. Four detection architectures‚ÄîYOLO11, RetinaNet, 
        Faster R-CNN, and Mask R-CNN‚Äîwere trained and tested on the KITTI dataset. While all models showed clear 
        improvements after fine-tuning, YOLO11 consistently delivered the strongest performance and the most stable 
        detections.
    </p>

    <p>
        When integrated into a tracking framework such as DeepSORT, the benefit of fine-tuning became even more 
        apparent. The fine-tuned YOLO11 model reduced ID switches by over <strong>70%</strong>, cut false positives by 
        half, and significantly increased precision, recall, and IDF1. These improvements show that the tracker 
        becomes more reliable simply because the detections it receives are cleaner, more accurate, and more 
        consistent over time.
    </p>

    <p>
        The results also highlight an important insight for real-world autonomous driving: 
        <strong>tracking cannot compensate for poor detections.</strong> Even advanced trackers like DeepSORT or 
        ByteTrack rely heavily on the bounding box quality, object confidence, and temporal consistency produced 
        by the detector. When the detector fails‚Äîmissing objects, producing noisy boxes, or fluctuating 
        confidence‚Äîthe tracker also becomes unstable.
    </p>

    <p>
        Overall, this study confirms that fine-tuning detection models on domain-specific datasets like KITTI 
        is essential for high-performance tracking. In safety-critical applications such as autonomous vehicles, 
        better detectors directly translate to more reliable perception, smoother object trajectories, and 
        reduced identity fragmentation.
    </p>


</section>


<!-- REFERENCES -->
<section id="references">
    <h2 class="section-title">References</h2>
    <ul>
        <li>Redmon et al., YOLO</li>
        <li>Lin et al., Focal Loss, RetinaNet</li>
        <li>Ren et al., Faster R-CNN</li>
        <li>He et al., Mask R-CNN</li>
        <li>Wojke et al., DeepSORT</li>
        <li>ByteTrack (2022)</li>
        <li>KITTI Dataset</li>
    </ul>
</section>

<!-- FOOTER -->
<footer class="text-center py-4 mt-5">
    <p>¬© 2025 Object Detection & Tracking Research Project</p>
</footer>

</div>

</body>
</html>
