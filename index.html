<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Object Detection & Tracking in Autonomous Driving</title>

    <!-- Bootstrap -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.2/dist/css/bootstrap.min.css" rel="stylesheet">

    <!-- Custom Style -->
    <style>
        body { background: #f8f9fa; }
        .section-title { font-weight: 700; font-size: 2rem; margin-top: 50px; }
        .subsection-title { font-weight: 600; font-size: 1.4rem; margin-top: 25px; }
        .img-block { width: 100%; border-radius: 10px; box-shadow: 0 2px 12px rgba(0,0,0,0.15); }
        .img-block-mini { width: 30%; border-radius: 10px; box-shadow: 0 2px 12px rgba(0,0,0,0.15); }
        .video-block { width: 100%; border-radius: 10px; margin: 10px 0; }
        .flowchart { width: 100%; padding: 10px; border-radius: 12px; background: white; }
    </style>
</head>

<body>

<!-- HERO -->
<header class="bg-dark text-white text-center py-5">
    <h1 class="display-4 fw-bold">Object Detection & Tracking System in Autonomous Driving</h1>
    <p class="lead">A Comparative Study of YOLO, RetinaNet, Faster R-CNN, Mask R-CNN + DeepSORT </p>
</header>

<div class="container py-4">

<!-- ABSTRACT -->
<section id="abstract">
    <h2 class="section-title">Abstract</h2>
    <p>
        This project investigates object detection and tracking performance in autonomous driving scenarios.
        Four detection models (YOLO11, RetinaNet, Faster R-CNN, Mask R-CNN) were fine-tuned on the KITTI object
        detection dataset and evaluated both on the KITTI detection and KITTI tracking dataset. Two tracking
        algorithms, DeepSORT, were used to analyze how detection accuracy affects multi-object tracking
        stability. Experimental results show that fine-tuned detectors significantly reduce ID switching and tracking
        fragmentation compared to pretrained models, confirming the strong coupling between detection quality and
        overall tracking performance.
    </p>
</section>

<!-- INTRODUCTION -->
<section id="intro">
    <h2 class="section-title">Introduction</h2>
    <p>
        Autonomous vehicles require robust detection and tracking of road users such as cars, pedestrians, and cyclists.
        While modern detectors achieve high accuracy on static images, their real-world performance within tracking
        pipelines is less explored. This project addresses the research gap:
        <strong>“How does the choice and fine-tuning of object detectors affect multi-object tracking performance?”</strong>
    </p>
</section>

<!-- DATASET -->
<section id="dataset">
    <h2 class="section-title">Dataset</h2>
    <h4 class="subsection-title">Why KITTI?</h4>
    <p>
      The <strong>KITTI Vision Benchmark Suite</strong> provides synchronized multi-sensor data from real driving scenarios: stereo RGB cameras, a 64-beam LiDAR scanner, and GPS/IMU for vehicle localization.  
      This rich sensor setup allows tasks beyond 2D detection — such as 3D object detection, depth estimation, odometry, and even sensor fusion studies. <br>
      The dataset is widely adopted in autonomous driving research, ensuring that results are comparable to the literature.
    </p>
    <img src="images/kitti.png" class="img-block-mini my-3" alt="KITTI detection sample">

    <h4 class="subsection-title">Dataset Structure & Contents</h4>
    <ul>
      <li><strong>Object Detection Benchmark:</strong> ~7481 training + ~7518 test images, annotated in 2D bounding-boxes + 3D box metadata. Class labels include Cars, Pedestrians, Cyclists, Trucks, Vans, Trams, etc.</li>
      <li><strong>Object Tracking Benchmark:</strong> 21 annotated sequences for training, 29 for testing. Ground-truth includes object track IDs, occlusion / truncation metadata, and 2D bounding boxes for each frame.</li>
      <li>Only “Car” and “Pedestrian” categories are evaluated in the official tracking benchmark to ensure sufficient data per class.</li>
      <li>Annotation format provides detailed information: occlusion level, truncation ratio, visibility, 3D position & size — enabling tasks beyond simple 2D detection.</li>
    </ul>

    <h4 class="subsection-title">KITTI Detection Dataset</h4>
    <ul>
        <li>Train: 5984 images</li>
        <li>Validation: 1497 images</li>
        <li>Classes: Car, Pedestrian, Cyclist</li>
    </ul>

    <img src="images/kitti_detection_sample1.png" class="img-block my-3" alt="KITTI detection sample">

    <h4 class="subsection-title">KITTI Tracking Dataset</h4>
    <ul>
        <li>20 sequences</li>
        <li>Variable frame count per sequence</li>
        <li>Used for Testing Detection & Tracking</li>
    </ul>

    <img src="images/kitti_tracking_sample.png" class="img-block my-3" alt="KITTI tracking sample">
</section>

<!-- MODELS -->
<section id="models">
    <h2 class="section-title">Object Detection Models</h2>

    <p>
        Modern autonomous driving systems require fast and accurate object detectors capable of identifying multiple
        classes across varying lighting, occlusion, and traffic conditions. In this project, we compare four
        state-of-the-art detection algorithms that represent both <strong>one-stage</strong> and <strong>two-stage</strong>
        detection paradigms. This selection allows us to analyze differences in accuracy, speed, complexity, and their
        impact on downstream tracking algorithms such as DeepSORT and ByteTrack.
    </p>

    <!-- MODEL IMAGES -->
    <div class="row text-center my-4">
        <div class="col-md-3">
            <img src="images/yolo_arch.png" class="img-block" alt="YOLO">
            <h5 class="mt-2">YOLO11 (One-Stage)</h5>
        </div>
        <div class="col-md-3">
            <img src="images/retina_arch.png" class="img-block" alt="RetinaNet">
            <h5 class="mt-2">RetinaNet (One-Stage)</h5>
        </div>
        <div class="col-md-3">
            <img src="images/frcnn_arch.png" class="img-block" alt="FRCNN">
            <h5 class="mt-2">Faster R-CNN (Two-Stage)</h5>
        </div>
        <div class="col-md-3">
            <img src="images/mrcnn_arch.png" class="img-block" alt="Mask R-CNN">
            <h5 class="mt-2">Mask R-CNN (Two-Stage)</h5>
        </div>
    </div>


    <!-- MODEL DESCRIPTIONS -->
    <h3 class="subsection-title">Model Descriptions & Motivation</h3>

    <h4 class="mt-4">1. YOLO11 (One-Stage Detector)</h4>
    <p>
        YOLO (“You Only Look Once”) is a real-time object detector that processes the entire image in a single forward pass. 
        YOLO11 represents the latest generation of this architecture, offering improved feature fusion, optimized anchor-free heads,
        and high-speed inference ideal for real-time autonomous driving.
        <br><strong>Why included?</strong> It provides the best speed–accuracy trade-off and is widely used in tracking pipelines.
    </p>

    <h4 class="mt-4">2. RetinaNet (One-Stage Detector)</h4>
    <p>
        RetinaNet introduced the <strong>Focal Loss</strong>, solving the class imbalance problem common in autonomous driving scenes 
        (many cars, few pedestrians/cyclists). It uses a Feature Pyramid Network (FPN) to detect small and medium objects more reliably.
        <br><strong>Why included?</strong> Represents earlier SOTA single-stage detectors and provides a strong baseline against YOLO.
    </p>

    <h4 class="mt-4">3. Faster R-CNN (Two-Stage Detector)</h4>
    <p>
        Faster R-CNN is a classic high-accuracy detector that uses a <strong>Region Proposal Network (RPN)</strong> to generate regions 
        before classification. It is slower but significantly more stable in complex scenes.
        <br><strong>Why included?</strong> Serves as a two-stage accuracy-focused baseline for comparison with one-stage methods.
    </p>

    <h4 class="mt-4">4. Mask R-CNN (Two-Stage Detector)</h4>
    <p>
        Mask R-CNN extends Faster R-CNN by adding an <strong>instance segmentation branch</strong>, improving spatial precision at 
        object boundaries. Even though this project uses only bounding boxes, the shared backbone provides strong feature representations.
        <br><strong>Why included?</strong> Represents more advanced two-stage models with higher modeling capacity.
    </p>


    <!-- COMPARISON TABLE -->
    <h3 class="subsection-title mt-5">Model Comparison Summary</h3>
    <table class="table table-bordered table-striped">
        <thead>
            <tr>
                <th>Model</th>
                <th>Type</th>
                <th>Speed</th>
                <th>Accuracy</th>
                <th>Strengths</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td>YOLO11</td>
                <td>One-Stage</td>
                <td>⭐⭐⭐⭐⭐</td>
                <td>⭐⭐⭐⭐</td>
                <td>Real-time, most stable for tracking</td>
            </tr>
            <tr>
                <td>RetinaNet</td>
                <td>One-Stage</td>
                <td>⭐⭐⭐</td>
                <td>⭐⭐⭐</td>
                <td>Focal Loss improves minority classes</td>
            </tr>
            <tr>
                <td>Faster R-CNN</td>
                <td>Two-Stage</td>
                <td>⭐⭐</td>
                <td>⭐⭐⭐⭐⭐</td>
                <td>Highly accurate proposals</td>
            </tr>
            <tr>
                <td>Mask R-CNN</td>
                <td>Two-Stage</td>
                <td>⭐</td>
                <td>⭐⭐⭐⭐⭐</td>
                <td>Best for occlusion & boundary precision</td>
            </tr>
        </tbody>
    </table>


    <!-- LOSS CURVES -->
    <h3 class="subsection-title mt-5">Training Loss Curves</h3>
    <p>
        Loss curves demonstrate whether each model converged properly during fine-tuning on the KITTI detection dataset.
        Models with steadily decreasing loss show stable learning and good generalization to validation data.
    </p>

    <div class="row text-center">
        <div class="col-md-3"><img src="images/yolo_loss.png" class="img-block"></div>
        <div class="col-md-3"><img src="images/retina_loss.png" class="img-block"></div>
        <div class="col-md-3"><img src="images/frcnn_loss.png" class="img-block"></div>
        <div class="col-md-3"><img src="images/mrcnn_loss.png" class="img-block"></div>
    </div>
</section>


<!-- DETECTION RESULTS -->
<section id="detection-results">
    <h2 class="section-title">Detection Results (KITTI)</h2>

    <h4 class="subsection-title">mAP Performance</h4>
    <p><strong>YOLO11 fine-tuned (given results):</strong></p>

    <table class="table table-bordered table-striped">
        <thead>
            <tr>
                <th>Model</th>
                <th>mAP50-95</th>
                <th>mAP50</th>
                <th>Car AP</th>
                <th>Pedestrian AP</th>
                <th>Cyclist AP</th>
            </tr>
        </thead>

        <tbody>
            <tr>
                <td>YOLO11 (FT)</td>
                <td>0.494</td>
                <td>0.765</td>
                <td>0.702</td>
                <td>0.369</td>
                <td>0.411</td>
            </tr>

            <!-- EMPTY ROWS FOR FUTURE RESULTS -->
            <tr><td>YOLO11 (Pretrained)</td><td colspan="5">Pending...</td></tr>
            <tr><td>RetinaNet (FT)</td><td colspan="5">Pending...</td></tr>
            <tr><td>Faster R-CNN (FT)</td><td colspan="5">Pending...</td></tr>
            <tr><td>Mask R-CNN (FT)</td><td colspan="5">Pending...</td></tr>
        </tbody>
    </table>

    <h4 class="subsection-title">Detection Examples</h4>
    <div class="row">
        <div class="col-md-3"><img src="images/yolo_detection.png" class="img-block"></div>
        <div class="col-md-3"><img src="images/retina_detection.png" class="img-block"></div>
        <div class="col-md-3"><img src="images/frcnn_detection.png" class="img-block"></div>
        <div class="col-md-3"><img src="images/mrcnn_detection.png" class="img-block"></div>
    </div>
</section>
<!-- TRACKING PIPELINE -->
<section id="tracking">
    <h2 class="section-title">Tracking Pipeline</h2>

    <img src="images/tracking_flowchart.svg" class="flowchart" alt="Tracking Flowchart">

    <h4 class="subsection-title">Overview</h4>
    <p>
        After evaluating four object detection models on KITTI Tracking, we selected two versions of YOLO11 
        (pretrained and fine-tuned) for multi-object tracking experiments. The goal was to analyze how detection
        quality influences the performance of a tracking algorithm in real driving scenes.
    </p>

    <h4 class="subsection-title">Why Multi-Object Tracking?</h4>
    <p>
        Autonomous vehicles must track the same object across consecutive frames. While object detection identifies 
        objects independently per image, tracking links detections over time to maintain consistent object identities.
        Tracking performance is highly sensitive to detection quality—poor detections lead to identity switches,
        false positives, and unstable trajectories.
    </p>

    <h4 class="subsection-title">Tracking Algorithm: DeepSORT</h4>
    <p>
        We employed <strong>DeepSORT</strong>, a widely used and reliable MOT algorithm. DeepSORT extends SORT by 
        adding appearance-based re-identification (ReID), making it more robust in crowded or occluded environments.
    </p>

    <ul>
        <li><strong>Appearance Embeddings (ReID):</strong> Each detected object is converted to a feature vector for identity matching.</li>
        <li><strong>Kalman Filter:</strong> Predicts object motion across frames even when detections are missing.</li>
        <li><strong>Hungarian Algorithm:</strong> Matches predictions with detections via IoU + appearance similarity.</li>
        <li><strong>ID Assignment:</strong> Maintains consistent ID numbers over time.</li>
    </ul>

    <p>
        DeepSORT is a strong baseline for autonomous driving systems due to its speed, simplicity, and reliable 
        identity preservation—especially important for safety-critical tasks.
    </p>

    <h4 class="subsection-title">Experiment Setup</h4>
    <p>
        We tested the tracker on KITTI Tracking Sequence <strong>0001</strong>, once using:
    </p>

    <ul>
        <li><strong>YOLO11 Pretrained</strong> detections</li>
        <li><strong>YOLO11 Fine-Tuned</strong> detections</li>
    </ul>

    <p>
        This allowed a controlled comparison of how fine-tuning detector models directly affects tracking behavior:
        fewer false detections → more accurate trajectories → reduced ID switches.
    </p>
</section>


<!-- TRACKING RESULTS -->
<section id="tracking-results">
    <h2 class="section-title">Tracking Results</h2>

    <h3 class="subsection-title">Qualitative Comparison (Sequence 0001)</h3>
    <p>
        Below are the tracking visualization videos generated from KITTI 0001. The improvement after fine-tuning 
        is clearly visible—IDs remain stable, tracks do not jump, and the tracker loses fewer objects.
    </p>

    <h5>YOLO11 (Pretrained) + DeepSORT</h5>
    <video class="video-block" controls autoplay>
        <source src="images/yolo_pretrained_deepsort.mp4" type="video/mp4">
    </video>

    <h5>YOLO11 (Fine-Tuned) + DeepSORT</h5>
    <video class="video-block" controls autoplay>
        <source src="images/yolo_finetuned_deepsort.mp4" type="video/mp4">
    </video>

    <img src="images/tracking_compare.png" class="img-block mt-3">

   <h3 class="subsection-title">Quantitative Metrics</h3>

<p>
    We evaluated the tracking performance on KITTI Sequence 0001 using standard Multi-Object Tracking (MOT) metrics.  
    These metrics quantify how well the tracker maintains object identities, minimizes false detections, and recovers 
    objects across frames. The results clearly show that <strong>fine-tuning the detector dramatically improves tracking performance</strong>.
</p>

<ul>
    <li><strong>MOTA (Multiple Object Tracking Accuracy):</strong> penalizes False Positives (FP), False Negatives (FN), and ID switches.</li>
    <li><strong>IDF1:</strong> measures how consistently object identities are preserved over time.</li>
    <li><strong>ID Switches:</strong> lower values indicate better identity stability.</li>
    <li><strong>FP / FN:</strong> false detections and missed detections.</li>
    <li><strong>Precision / Recall:</strong> overall detection quality during tracking.</li>
</ul>

<table class="table table-bordered">
    <thead>
        <tr>
            <th>Setup</th>
            <th>ID Switch</th>
            <th>FP</th>
            <th>FN</th>
            <th>Recall</th>
            <th>Precision</th>
            <th>MOTA</th>
            <th>IDF1</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>YOLO11 Pretrained + DeepSORT</td>
            <td>176</td>
            <td>6560</td>
            <td>2081</td>
            <td>0.4873</td>
            <td>0.2317</td>
            <td>-1.9099</td>
            <td>0.314</td>
        </tr>

        <tr>
            <td>YOLO11 Fine-Tuned + DeepSORT</td>
            <td>49</td>
            <td>3206</td>
            <td>1771</td>
            <td>0.5438</td>
            <td>0.3970</td>
            <td>-0.6587</td>
            <td>0.459</td>
        </tr>
    </tbody>
</table>

<p>
    The fine-tuned YOLO11 model provides a much cleaner input to the tracker:
</p>

<ul>
    <li><strong>ID switches dropped from 176 → 49</strong> (a 72% improvement), meaning identities remain stable.</li>
    <li><strong>False Positives reduced by 51%</strong> (6560 → 3206), avoiding phantom detections.</li>
    <li><strong>False Negatives reduced</strong> (2081 → 1771), so fewer objects are missed.</li>
    <li><strong>Precision improved from 0.23 → 0.40</strong>, meaning detections are more reliable.</li>
    <li><strong>Recall improved from 0.49 → 0.54</strong>, indicating more objects are successfully tracked.</li>
    <li><strong>IDF1 improved from 0.314 → 0.459</strong>, showing stronger identity consistency.</li>
</ul>

<p>
    Although MOTA remains negative (common on KITTI when FP is high), the improvement from <strong>-1.91 to -0.65</strong>
    indicates that the tracker benefits substantially from the cleaner detections of the fine-tuned model.
</p>

     <div class="row text-center">
        <div class="col-md-4"><img src="images/compare1.png" class="img-block"></div>
        <div class="col-md-4"><img src="images/compare2.png" class="img-block"></div>
        <div class="col-md-4"><img src="images/compare3.png" class="img-block"></div>
    </div>
    
<p>
    Overall, these results confirm that <strong>detection quality is the dominant factor in multi-object tracking</strong>.  
    Fine-tuned YOLO11 produces more accurate bounding boxes and fewer noisy detections, resulting in significantly 
    more stable and reliable tracking performance.
</p>


    <p>
        Fine-tuning the detector reduced ID switches by <strong>~72%</strong> and significantly improved precision and IDF1, 
        showing that stronger detections directly improve tracking stability.
    </p>
</section>

<!-- CONCLUSION -->
<section id="conclusion">
    <h2 class="section-title">Conclusion</h2>
    <p>
        Fine-tuned detection models significantly improve multi-object tracking quality in autonomous driving data.
        YOLO11 achieved the best overall performance, and when used with DeepSORT, it reduced ID switching and improved
        track continuity. The results confirm that high-quality detections directly lead to better tracking stability.
    </p>
</section>

<!-- REFERENCES -->
<section id="references">
    <h2 class="section-title">References</h2>
    <ul>
        <li>Redmon et al., YOLO</li>
        <li>Lin et al., Focal Loss, RetinaNet</li>
        <li>Ren et al., Faster R-CNN</li>
        <li>He et al., Mask R-CNN</li>
        <li>Wojke et al., DeepSORT</li>
        <li>ByteTrack (2022)</li>
        <li>KITTI Dataset</li>
    </ul>
</section>

<!-- FOOTER -->
<footer class="text-center py-4 mt-5">
    <p>© 2025 Object Detection & Tracking Research Project</p>
</footer>

</div>

</body>
</html>
