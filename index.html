<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Object Detection & Tracking in Autonomous Driving</title>

    <!-- Bootstrap -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.2/dist/css/bootstrap.min.css" rel="stylesheet">

    <!-- Custom Style -->
    <style>
        body { background: #f8f9fa; }
        .section-title { font-weight: 700; font-size: 2rem; margin-top: 50px; }
        .subsection-title { font-weight: 600; font-size: 1.4rem; margin-top: 25px; }
        .img-block { width: 100%; border-radius: 10px; box-shadow: 0 2px 12px rgba(0,0,0,0.15); }
        .video-block { width: 100%; border-radius: 10px; margin: 10px 0; }
        .flowchart { width: 100%; padding: 10px; border-radius: 12px; background: white; }
    </style>
</head>

<body>

<!-- HERO -->
<header class="bg-dark text-white text-center py-5">
    <h1 class="display-4 fw-bold">Object Detection & Tracking System in Autonomous Driving</h1>
    <p class="lead">A Comparative Study of YOLO, RetinaNet, Faster R-CNN, Mask R-CNN + DeepSORT </p>
</header>

<div class="container py-4">

<!-- ABSTRACT -->
<section id="abstract">
    <h2 class="section-title">Abstract</h2>
    <p>
        This project investigates object detection and tracking performance in autonomous driving scenarios.
        Four detection models (YOLO11, RetinaNet, Faster R-CNN, Mask R-CNN) were fine-tuned on the KITTI object
        detection dataset and evaluated both on the KITTI detection and KITTI tracking dataset. Two tracking
        algorithms, DeepSORT, were used to analyze how detection accuracy affects multi-object tracking
        stability. Experimental results show that fine-tuned detectors significantly reduce ID switching and tracking
        fragmentation compared to pretrained models, confirming the strong coupling between detection quality and
        overall tracking performance.
    </p>
</section>

<!-- INTRODUCTION -->
<section id="intro">
    <h2 class="section-title">Introduction</h2>
    <p>
        Autonomous vehicles require robust detection and tracking of road users such as cars, pedestrians, and cyclists.
        While modern detectors achieve high accuracy on static images, their real-world performance within tracking
        pipelines is less explored. This project addresses the research gap:
        <strong>“How does the choice and fine-tuning of object detectors affect multi-object tracking performance?”</strong>
    </p>
</section>

<!-- DATASET -->
<section id="dataset">
    <h2 class="section-title">Dataset</h2>
    <h4 class="subsection-title">Why KITTI?</h4>
    <p>
      The <strong>KITTI Vision Benchmark Suite</strong> provides synchronized multi-sensor data from real driving scenarios: stereo RGB cameras, a 64-beam LiDAR scanner, and GPS/IMU for vehicle localization.  
      This rich sensor setup allows tasks beyond 2D detection — such as 3D object detection, depth estimation, odometry, and even sensor fusion studies. <br>
      The dataset is widely adopted in autonomous driving research, ensuring that results are comparable to the literature.
    </p>
    <img src="images/kitti.png" class="img-block my-3" alt="KITTI detection sample">

    <h4 class="subsection-title">Dataset Structure & Contents</h4>
    <ul>
      <li><strong>Object Detection Benchmark:</strong> ~7481 training + ~7518 test images, annotated in 2D bounding-boxes + 3D box metadata. Class labels include Cars, Pedestrians, Cyclists, Trucks, Vans, Trams, etc.</li>
      <li><strong>Object Tracking Benchmark:</strong> 21 annotated sequences for training, 29 for testing. Ground-truth includes object track IDs, occlusion / truncation metadata, and 2D bounding boxes for each frame.</li>
      <li>Only “Car” and “Pedestrian” categories are evaluated in the official tracking benchmark to ensure sufficient data per class.</li>
      <li>Annotation format provides detailed information: occlusion level, truncation ratio, visibility, 3D position & size — enabling tasks beyond simple 2D detection.</li>
    </ul>

    <h4 class="subsection-title">KITTI Detection Dataset</h4>
    <ul>
        <li>Train: 5984 images</li>
        <li>Validation: 1497 images</li>
        <li>Classes: Car, Pedestrian, Cyclist</li>
    </ul>

    <img src="images/kitti_detection_sample.png" class="img-block my-3" alt="KITTI detection sample">

    <h4 class="subsection-title">KITTI Tracking Dataset</h4>
    <ul>
        <li>20 sequences</li>
        <li>Variable frame count per sequence</li>
        <li>Used for Testing Detection & Tracking</li>
    </ul>

    <img src="images/kitti_tracking_sample.png" class="img-block my-3" alt="KITTI tracking sample">
</section>

<!-- MODELS -->
<section id="models">
    <h2 class="section-title">Object Detection Models</h2>

    <div class="row text-center">
        <div class="col-md-3"><img src="images/yolo_arch.png" class="img-block" alt="YOLO"></div>
        <div class="col-md-3"><img src="images/retina_arch.png" class="img-block" alt="RetinaNet"></div>
        <div class="col-md-3"><img src="images/frcnn_arch.png" class="img-block" alt="FRCNN"></div>
        <div class="col-md-3"><img src="images/mrcnn_arch.png" class="img-block" alt="Mask R-CNN"></div>
    </div>

    <h3 class="subsection-title">Training Loss Curves</h3>
    <div class="row text-center">
        <div class="col-md-3"><img src="images/yolo_loss.png" class="img-block"></div>
        <div class="col-md-3"><img src="images/retina_loss.png" class="img-block"></div>
        <div class="col-md-3"><img src="images/frcnn_loss.png" class="img-block"></div>
        <div class="col-md-3"><img src="images/mrcnn_loss.png" class="img-block"></div>
    </div>
</section>

<!-- DETECTION RESULTS -->
<section id="detection-results">
    <h2 class="section-title">Detection Results (KITTI)</h2>

    <h4 class="subsection-title">mAP Performance</h4>
    <p><strong>YOLO11 fine-tuned (given results):</strong></p>

    <table class="table table-bordered table-striped">
        <thead>
            <tr>
                <th>Model</th>
                <th>mAP50-95</th>
                <th>mAP50</th>
                <th>Car AP</th>
                <th>Pedestrian AP</th>
                <th>Cyclist AP</th>
            </tr>
        </thead>

        <tbody>
            <tr>
                <td>YOLO11 (FT)</td>
                <td>0.494</td>
                <td>0.765</td>
                <td>0.702</td>
                <td>0.369</td>
                <td>0.411</td>
            </tr>

            <!-- EMPTY ROWS FOR FUTURE RESULTS -->
            <tr><td>YOLO11 (Pretrained)</td><td colspan="5">Pending...</td></tr>
            <tr><td>RetinaNet (FT)</td><td colspan="5">Pending...</td></tr>
            <tr><td>Faster R-CNN (FT)</td><td colspan="5">Pending...</td></tr>
            <tr><td>Mask R-CNN (FT)</td><td colspan="5">Pending...</td></tr>
        </tbody>
    </table>

    <h4 class="subsection-title">Detection Examples</h4>
    <div class="row">
        <div class="col-md-3"><img src="images/yolo_detection.png" class="img-block"></div>
        <div class="col-md-3"><img src="images/retina_detection.png" class="img-block"></div>
        <div class="col-md-3"><img src="images/frcnn_detection.png" class="img-block"></div>
        <div class="col-md-3"><img src="images/mrcnn_detection.png" class="img-block"></div>
    </div>
</section>

<!-- TRACKING PIPELINE -->
<section id="tracking">
    <h2 class="section-title">Tracking Pipeline</h2>

    <img src="images/tracking_flowchart.svg" class="flowchart" alt="Tracking Flowchart">

    <h4 class="subsection-title">Tracker Overview</h4>
    <p>
        Two state-of-the-art multi-object tracking algorithms were used:
    </p>
    <ul>
        <li><strong>DeepSORT</strong>: Appearance-based tracking using ReID + Kalman filter + Hungarian matching</li>
        <li><strong>ByteTrack</strong>: High-performance tracker robust to low detection confidence</li>
    </ul>
</section>

<!-- TRACKING RESULTS -->
<section id="tracking-results">
    <h2 class="section-title">Tracking Results</h2>

    <h3 class="subsection-title">Qualitative Comparison</h3>

    <h5>YOLO11 (Pretrained) + DeepSORT</h5>
    <video class="video-block" controls>
        <source src="videos/yolo_pretrained_deepsort.mp4" type="video/mp4">
    </video>

    <h5>YOLO11 (Fine-Tuned) + DeepSORT</h5>
    <video class="video-block" controls>
        <source src="videos/yolo_finetuned_deepsort.mp4" type="video/mp4">
    </video>

    <img src="images/tracking_compare.png" class="img-block mt-3">

    <h3 class="subsection-title">Tracking Metrics</h3>

    <table class="table table-bordered">
        <thead>
            <tr>
                <th>Setup</th>
                <th>ID Switch</th>
                <th>Fragmentation</th>
                <th>FP</th>
                <th>FN</th>
                <th>Recall</th>
                <th>Precision</th>
            </tr>
        </thead>
        <tbody>
            <tr><td>YOLO Pretrained + DeepSORT</td><td colspan="6">Pending...</td></tr>
            <tr><td>YOLO Fine-Tuned + DeepSORT</td><td colspan="6">Pending...</td></tr>
        </tbody>
    </table>
</section>

<!-- CONCLUSION -->
<section id="conclusion">
    <h2 class="section-title">Conclusion</h2>
    <p>
        Fine-tuned detection models significantly improve multi-object tracking quality in autonomous driving data.
        YOLO11 achieved the best overall performance, and when used with DeepSORT, it reduced ID switching and improved
        track continuity. The results confirm that high-quality detections directly lead to better tracking stability.
    </p>
</section>

<!-- REFERENCES -->
<section id="references">
    <h2 class="section-title">References</h2>
    <ul>
        <li>Redmon et al., YOLO</li>
        <li>Lin et al., Focal Loss, RetinaNet</li>
        <li>Ren et al., Faster R-CNN</li>
        <li>He et al., Mask R-CNN</li>
        <li>Wojke et al., DeepSORT</li>
        <li>ByteTrack (2022)</li>
        <li>KITTI Dataset</li>
    </ul>
</section>

<!-- FOOTER -->
<footer class="text-center py-4 mt-5">
    <p>© 2025 Object Detection & Tracking Research Project</p>
</footer>

</div>

</body>
</html>
